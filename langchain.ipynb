{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c684f967-f246-442e-ad00-3f55ef5e5ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\naveena\\anaconda3\\lib\\site-packages (0.3.18)\n",
      "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain) (0.3.34)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain) (0.3.6)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain) (0.3.7)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain) (2.10.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain) (2.0.38)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Collecting numpy<2,>=1.26.4 (from langchain)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.34->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.34->langchain) (23.2)\n",
      "Collecting typing-extensions>=4.7 (from langchain-core<1.0.0,>=0.3.34->langchain)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2020.6.20)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain) (2.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n",
      "Using cached numpy-1.26.4-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: typing-extensions, numpy\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.5.0\n",
      "    Uninstalling typing_extensions-4.5.0:\n",
      "      Successfully uninstalled typing_extensions-4.5.0\n",
      "Successfully installed numpy-1.26.4 typing-extensions-4.12.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyfume 0.3.4 requires numpy==1.24.4, but you have numpy 1.26.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66fe02b7-f693-4c34-9153-2bad2ff74e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_core in c:\\users\\naveena\\anaconda3\\lib\\site-packages (0.3.34)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain_core) (0.3.7)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain_core) (8.2.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain_core) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain_core) (6.0.1)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain_core) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain_core) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain_core) (2.10.6)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain_core) (2.1)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_core) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_core) (3.10.15)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_core) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_core) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain_core) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain_core) (2.27.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_core) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_core) (2020.6.20)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_core) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_core) (2.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_core) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain_core) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain_core) (1.25.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_core) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b771d965-5be0-408e-8782-65884cc2d0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_ollama in c:\\users\\naveena\\anaconda3\\lib\\site-packages (0.2.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.33 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain_ollama) (0.3.34)\n",
      "Requirement already satisfied: ollama<1,>=0.4.4 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain_ollama) (0.4.7)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_ollama) (0.3.7)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_ollama) (8.2.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_ollama) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_ollama) (6.0.1)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_ollama) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_ollama) (4.12.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain_ollama) (2.10.6)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from ollama<1,>=0.4.4->langchain_ollama) (0.28.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (4.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (2020.6.20)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (2.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (2.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (3.10.15)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain_ollama) (1.25.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from anyio->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain_ollama) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install langchain_ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9fb67ea-1220-4616-a027-1782d168720b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing_extensions in c:\\users\\naveena\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade typing_extensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b6c05e-4e7f-46b7-abd5-9052fcff5f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "323bf5dd-08c0-4065-b4f5-3a1a6bb44e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70090b46-5ed6-4dfd-89d2-556675426150",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(\n",
    "    model = 'llama2:latest',\n",
    "    base_url = 'http://localhost:11434',\n",
    "    temperature = 0.3,\n",
    "    format=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1dc79f8-3ff7-4e8a-8a56-c2628177f4a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"\\nLangChain is a term that can have different meanings depending on the context. Here are some possible interpretations of LangChain:\\n\\n1. Language chain: In linguistics, a language chain refers to a sequence of languages spoken by a group of people or a community over time. For example, a person may speak English at home, Spanish at work, and their native language with their family. The language chain can help us understand how language use and language contact can vary across different contexts and groups.\\n2. Linguistic chain: In linguistics, a linguistic chain refers to a series of languages that are related through a common ancestral language or language family. For example, the Romance languages (Spanish, French, Italian, etc.) are part of the Indo-European language family and can be connected through a linguistic chain.\\n3. Chain of translation: In translation studies, a LangChain can refer to a sequence of translations or interpretations of a text across different cultures or languages. For example, a poem translated from Spanish into English and then into Arabic could form a LangChain of translations.\\n4. Language chain reaction: In the context of language learning and teaching, a LangChain can refer to a series of language-related activities or tasks that are connected in some way. For example, a teacher might create a LangChain of activities that start with a vocabulary lesson, move on to a grammar exercise, and end with a conversation practice.\\n5. Chain of communication: In communication studies, a LangChain can refer to a sequence of messages or communicative acts that are connected in some way. For example, a chain of emails between two coworkers could form a LangChain of communication.\\n\\nWithout more context, it's difficult to determine which of these meanings is most relevant to your question. If you have any additional information or clarification, I'd be happy to try and help further!\", additional_kwargs={}, response_metadata={'model': 'llama2:latest', 'created_at': '2025-02-23T12:04:44.5854691Z', 'done': True, 'done_reason': 'stop', 'total_duration': 67517530000, 'load_duration': 8354671100, 'prompt_eval_count': 24, 'prompt_eval_duration': 2611000000, 'eval_count': 413, 'eval_duration': 56540000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-c5bf0645-2029-4ee6-b8b7-b281c0a98e0e-0', usage_metadata={'input_tokens': 24, 'output_tokens': 413, 'total_tokens': 437})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke('what is langchain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87c59cdb-cbdb-4536-b80f-a2c922587a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    " you explain things to people like they are 5 years old\n",
    " \"\"\"\n",
    "user_prompt = \"what is langchian?\"\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a6e1eb-4e30-41e4-b9bc-f2f781882e94",
   "metadata": {},
   "source": [
    "## chains, prompts and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "542b50bc-8914-4f45-a04f-afe7ce926da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1281afda-4fb5-4cd7-9f3e-997c61e76728",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content = system_prompt),\n",
    "    HumanMessage(content = user_prompt)              \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "552eab20-1a48-4f29-8e1e-778b7b756e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke(messages)  # is it possible to chain this message with llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1be13f1a-858f-4b26-b020-394ae0d80bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=' Oh, wow! *excited* Langchian is a super cool thing! *giggles* It\\'s like, imagine you have a special language that only you and your friends can understand. *nods*\\n\\nSo, you know how we have words like \"dog\" and \"cat\"? *points* Well, langchian is like a secret code word that only you and your friends know. *winks* It\\'s like having your very own special language! *giggles*\\n\\nImagine you\\'re playing with your friend in the park, and you want to tell them something super duper cool, but you don\\'t want anyone else to hear. *nods* You can use langchian to tell them a secret! *excited* It\\'s like having your very own secret handshake! *giggles*\\n\\nAnd the best part is, you can make up your own langchian words! *excited* Like, you could say \"flumplenook\" instead of \"dog\", and your friend would know exactly what you mean! *nods* It\\'s like having your very own secret language! *giggles*\\n\\nSo, that\\'s what langchian is! *excited* Isn\\'t it fun? *giggles* Do you want to make up some langchian words with me? *smiles*' additional_kwargs={} response_metadata={'model': 'llama2:latest', 'created_at': '2025-02-23T12:05:23.5137353Z', 'done': True, 'done_reason': 'stop', 'total_duration': 38891896800, 'load_duration': 14422000, 'prompt_eval_count': 40, 'prompt_eval_duration': 643000000, 'eval_count': 304, 'eval_duration': 38176000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-fddd173d-79aa-4c8e-94b8-cd59d3ffd96c-0' usage_metadata={'input_tokens': 40, 'output_tokens': 304, 'total_tokens': 344}\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e547073-efa9-4906-ae48-3f5c56dd0dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ae2fcde-fb4b-45ce-95dc-02b8e2b44363",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are Helpful assistant that explains AI topics. Given the following input:\n",
    "{topic}\n",
    "Provide an explanation of the given topic.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = ['topic'],\n",
    "    template = prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31703097-6472-46ff-a28a-3feafe858680",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c12992da-494f-4447-9e56-4d5e0c9850df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Great! I\\'d be happy to help explain what LangChain is.\\n\\nLangChain is a term used in the field of natural language processing (NLP) and machine learning (ML). It refers to a type of language model that is designed to process and generate text at various levels of abstraction, from individual words and phrases to higher-level concepts and structures.\\n\\nThe term \"LangChain\" was coined by researchers at the University of California, Berkeley in 2019, as part of their work on a new type of language model called the \"Transformer.\" The Transformer architecture is designed to be more flexible and powerful than traditional language models, which are typically trained to perform a single task such as language translation or text summarization.\\n\\nLangChain models are trained on large datasets of text and are capable of learning a wide range of linguistic patterns and structures, including grammar, syntax, semantics, and pragmatics. They can be used for a variety of NLP tasks, such as language translation, text generation, and question answering.\\n\\nOne key advantage of LangChain models is their ability to capture long-range dependencies in text. Unlike traditional language models, which are limited to processing the input text one word or one sentence at a time, LangChain models can consider the entire input sequence when making predictions. This allows them to generate more coherent and contextually appropriate output.\\n\\nOverall, LangChain is an exciting area of research in NLP that has the potential to significantly improve the performance of language models and other NLP tasks.', additional_kwargs={}, response_metadata={'model': 'llama2:latest', 'created_at': '2025-02-23T12:06:06.7578671Z', 'done': True, 'done_reason': 'stop', 'total_duration': 43181169600, 'load_duration': 14023600, 'prompt_eval_count': 54, 'prompt_eval_duration': 557000000, 'eval_count': 335, 'eval_duration': 42609000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-cfbd774d-ed3c-479b-9787-0bd7afa274da-0', usage_metadata={'input_tokens': 54, 'output_tokens': 335, 'total_tokens': 389})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({'topic': 'what is langchain'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b71ccc3d-3fd1-485a-b229-370b60abe22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade --quiet youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b307331e-f778-49c1-acfd-1b6dea259773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "888c2174-e6b9-4e74-a642-05ab16aadc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = YoutubeLoader.from_youtube_url(\"https://youtu.be/8BV9TW490nQ?si=lFVcy5SuQ5dBueU5\", add_video_info = False)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0924de33-6e01-47ee-a2c3-ef3fdb4ec381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"in this video you'll learn everything you need to know about Lang chain in seven easy steps I've broken down the framework into key components and here's an overview of the image I'm going to try to put in your mind in the next 40 minutes there is a lot to unpack but I'll walk you through the components one by one and I've included coding examples with every step I'll start off by asking why length chain and I'll give you a reason why you want to learn this and an overview of the framework then I'll show you how to build chains using prompts and ERS and we'll dive into the runable protocol and Lang chain expression language then we'll look at Splitters and retrievers and we'll build a rag chain finally I'll walk you through how to use tools and how to build an agent with access to Tools in this video I'll keep my focus on the core Lang chain framework and build the foundation for advanced use cases so I'll save Lang graph Lang Smith and Lang surf for separate videos if you're new to Lang chain this tutorial will serve as a quick start and if you're already already familiar with the framework you can use the tutorial to tie up some loose ends I'll share a link to this interactive map of Lang chain to the code and to a written tutorial below this video let's get started I'm going to start off by giving you an overview of the linkchain framework what is it why would you want to use it and then I'm going to show you how to get started developing in a collab notebook so what is Lang chain Lang chain is an open open source framework for developing llm applications an engine allows developers to combine llms with external sources of computation and data and the framework further simplifies the deployment of the llm applications by providing the needed tools such as tools for inspection monitoring and evaluation the deployment and monitoring tools are part of Lang Smith and Lang serf I'm not going to cover that here I'm going to cover the core Library so Lang chain combines llms with external sources of computation and data how does that work suppose that a user wants to ask a question related to some documents or some data that the user has access to now this question will be sent to the llm just like when you're interacting with chat DBT but you also need to fetch the context needed to answer the question from your own data or your own documents and to do that a vector representation of the initial question from the user is used to fetch all the relevant context needed to answer the question typically from a vector database this context is then fed to the llm alongside with the initial question and now the llm has everything it needs to answer the question in the right context and the magical thing about this is that we can do more than just answer the question we can actually take actions based on the input from the end user and we can do that because Lang chain allows us to build agents out of llms and agents are created by by combining llms with tools this will give the llms access to external sources of computation and data so what can we build with Lang chain when it comes to the types of applications you can build with llms this guise the limit from what I see most llm applications that have been built over the last year fall into one or three categories chatbots racun applications or agent systems so the chat BS are nothing new they're used for marketing customer support different types of education and creating assistance what is new is that these chat Bots can now be made more intelligent using llms then we have the rag Q&A applications and this is the magic that Lang chain and llama index made people aware of a year ago so these are the applications that are used for summarizing large documents doing data analysis and generating code by referencing repositories and then we have what we can call agent system both multi-agent systems and systems where we Loop humans into complex interactions with agents and this is what Lang graph is used for and Lang graph is a rather new component in the Lang chain ecosystem the number of potential applications of agent systems is virtually Limitless they're being implemented for Supply Chain management Revenue optimization operations management and so on all right so let's zoom out a bit and have a look at why you want to learn langing so so there are at least two good reasons for learning Lang chain so first of all Lang chain facilitates the transitioning to the next generation of programming languages and what do I mean by that so if we have a look at programming language hierarchies this is what it typically looks like so at the bottom of the pyramid you will have machine language on top of that you will have Assembly Language then you have the higher level languages those are the languages that you are used to program coming in so python Java SQL R C++ and so on and we've been moving up the pyramid ever since programming became a thing and right now we are at the final step the highest level of all languages which is natural language because with the llms we don't have to write code anymore we can have the llms write code for us by giving natural language as an input and the development of these pipelines that allows us to turn natural language into programs is exactly what Lang chain facilitates so we can create pipelines or chains as they are called that allows us to input natural language and get something of value in return like a report an analysis or computer program and with the current generation of best-in-class llms we can already write executable code code that works so this is not something that belongs to the Future it's already here now the second reason you you might want to use Lang chain is really not Lang chain itself but what's being built on top of Lang chain which is Lang graph now Lang chain was late to the multi-agent game you had autogen you had crew AI before that but what langra can do is really remarkable for instance Lang graph allows us to build complex interactions between agents and humans and this is needed especially in big organizations where you have to guard the access to certain files and so on but Lang graph also allows us to build multi-agent systems like autogen or crew II and the use cases for this are becoming more clear every day we need to be able to set up arbitrary communication patterns between agents we also need to do divide and conquer just like humans we can't have an llm do everything so we want to build specialized agents and we want them to be able to perform tasks in an asynchronous way all right let's get started developing the first thing you want to do is you want to fetch the API key from the llm service you want to use so in this tutorial I'm going to use openi and anthropic but Lang chain supports a wide array of llm providers so you can just choose your favorite one once you have grabbed your API key you want to put it in a EnV file all right so here on the collab notebook I have already installed the needed libraries here I'm going to install python. EnV to fetch API keys from the EnV file I'm going to install Lang chain of course Lang chain Community Omi and anthropic and I have uploaded the EnV file from my local machine to the cab notebook and now I'm ready to load the environment variables so I'm loading the API keys and once I've done that I can use chat anthropic and chat Omi to connect to anthropic and Omi and then I'm going to make a test call to GPT 4 and ask what is Lang chain and I simply make an LM call by using the invoke method and here we have an answer from gbt 4 this is very similar to what you would do if you were just calling gbt 4 directly through the open API and of course we can just switch out the llm now I'm calling clot 3 and here I also get a response so I can also easily prpt the llm with a system message and a user message at the same time and I'll just send the two messages in a list to the llm and this will allow you to do some basic prompt engineering but we can do a lot more advanced stuff than this once we start utilizing prom templates and building chains all right so let's dive into chains prompts and loaders and build our first chain I'm going to walk you through what a chain is and give you an analogy to think of then I'm going to work you through prom templates and how to use document loaders so a chain in Lang chain is a sequence of interconnected components that process a user's query utilizing one or more llms to generate and deliver some valuable output and I'm going to get into what those components are in a bit but let me just give you examples of the valuable output that we can generate with the chain so examples of valuable output could be an answer to a specific question it could be a summary of some document it could be a recommendation insights from data a piece of executable code or some Json for an API call and with the chains we can automate the generation of this value now what about the components the components of a chain are typically proms llms or what we call chat models output passes that passes the output of the llm tools that allow the llms to extract additional information from apis or run some code this is what turns llms into agents and general functions and in Lang chain these components are so-called runnables that can be chained together to produce a pipeline and this is one way to think of chains as pipelines in the world of llms so an analogy would be a data pipeline in a data pipeline we typically input some raw data and we get some nice clean data out in a table format and the data cleaning and transformation happening in the pipeline is valuable similarly in an llm chain we input a query and we get some valuable output in return and here the query is turned into value using llm calls functions and additional data and just like with data pipelines we can automate this process with cicd continuous integration and continuous delivery and this is what Lang chain as a framework can help you do so not only can you build the chains but you can also test the chains deploy them monitor the chains and evaluate the output and this can be done with Lang Smith and Lang surf so Lang chain has several built-in modules or classes that helps you create chains efficiently and one of those modules are prompt templates so a prompt for an llm is a set of instructions provided by a user to guide the model's response and promt templates in Lang chain are predefined prompt recipes these recipes are very helpful when you want to create chains fast another feature of Lang chain is that you have so-called document loaders a document is a piece of text and Associated metadata and the Lang chain document loaders provide a load method for loading data as documents from a source into a chain now let's head over to a collab notebook and set up a simple chain I'm going to import prompt template from Lang chain prompts and then I'm going to formulate the prompt to the llm as a string with a placeholder in this case with the variable topic and then I set up the prompt using the prompt template I just imported from Lang chain an input variable that I'm going to call topic and then the string the template itself and this is a simple example of a prompt that now allows me to comp OS a chain and I do that using the pipe operator so I can take the prompt and the language model LL mgbt 4 and assemble a chain using those two components and now I can invoke the chain with an input variable in this case topic so this was a very simple example of assembling a chain using the pipe operator let's try to make this a bit more advanced I'm going to create a chain that will transcribe one of my YouTube videos so I'm going to pip install the YouTube transcript API and then I can use one of the loaders that is spilled into L chain the YouTube loader so I'll import YouTube loader from Lang chain Community document loaders and then I can simply pass the URL to one of my videos or any video to the loader and have the loader extract the transcript from the video now if we have a look at what the loader extracts you can see that we get a list of documents if you want the transcript you can simply extract the page content from the documents or you can simply pass the documents to the chain directly so fetching the raw transcript would be as simple as just fetching the single document in this list and then extracting the page content all right so let's set up a chain that will summarize YouTube videos given a transcript so here I'm going to inject the video transcript into the prom template so this is going going to be my input variable and then I just set up the chain like I did before with the pipe operator and I'm going to invoke it with the video transcript and note that I can just pass the documents I don't have to extract the raw transcript and here we have a nice summary of my YouTube video so Lang chain also has a number of build-in chains that you can use for stuff like this for instance the create stuff documents chain and this chain will take a list of documents and pass them all in as context in the prom template pretty much what we did before just without using the buil-in chain this was just a very basic introduction to building chains now in order to build more advanced chains we need to understand Lang chains runnable protocol and Lang chain expression language in more detail let's now try to dive deeper into Lang chain expression language and the runnable protocol the Lang chain expression language simplifies building complex chains from basic components and we do that using the pipe operator that allows us to chain different components together feeding the output from one component to the next so a simple example of a chain composed in this way would be a prompt combined with a model and an output passer and these components are so-called runnables think of Lang chain expression language as a declarative way of composing these so-called runnables into chains and I'll get into what a runnable is in a second here we have a step-by-step example of what a chain looks like we have an input to the chain which is typically a dictionary that is sent to a prompt then a prompt value is sent to the or the chat model then the chat model will return a chat message that we want to pass and we typically want to use the passer to extract a string from the chat message and this will be the output of the chain this is one example of a basic chain and L chain allows us to chain multiple chains together like this so we might want to use the string output of the chain and send that to a new chain that will return a different output so what are runnables let's try to understand the runnable protocol this definition is directly from Lang chain a runnable is a unit of work that can be invoked batched streamed transformed and composed and the chains we built with Lang chain and the components of those chains are runnables and apart from Lang chain native components being runnables we can actually also pass in random functions into chain and those functions will then turn into runnables I'll show you that in an example when we get to the code and these chains can be invoked that means we execute the chain we can call batch on them if we have multiple inputs and we can stream the chains there are some core runnable objects in Lang chain that you want to understand in order to fully appreciate the Lang chain expression language these four objects are the most important ones runnable sequence runnable Lambda runnable pass through and runnable parallel so let's start from the top a runnable sequence in Lang chain is a class that chains together multiple runnable components and it ensures that each component processes its input and sequentially passes its output to the next component in general the change you're building in Lang chain will be of type runnable sequence now what about runnable Lambda runnable Lambda is a class that turns a python callable like a function into a runnable component and this allows us to integrate arbitary functions into chains as in the example I just showed you then we have runnable pass through and a runnable pass through in Lang chain is a class that either passes its input through unchanged or adds additional keys to the output in the first case it acts as a placeholder and in the second case it allows us to do flexible Integrations into sequences where we need to modify the input and finally we have runnable parallel and a runnable parallel in L chain is a class that runs multiple runnables concurrently and this allows you to do branching where you have two chains run the same input but return different outputs now let's see how this works by looking at some code examples all right so I'm going to use the same symbol example throughout this section I'm going to have an llm summarize AI Concepts here I have a prompt that tells the llm to summarize an AI concept and then I'm going to inject a context into the prompt the first thing I'm going to do is I'm going to create a simple chain using the pipe operator we already did it once now I'm going to add an additional element the output passer so the chain is composed of three elements the summarized prompt the llm and the output passer and when we invoke the chain we get the text output now remember that chains are of type runnable sequence and we can check the type of the chain that we just created to see if that is the case so I'm just going to print the type of the chain and here we can see that it is indeed of type runnable sequence now let's try to use a runnable Lambda here we have the same summarized chain as above but now with an additional element so I have a function that will calculate the length of the summary and then I'm wrapping this function in a runable Lambda this allows me to extend the chain we had before with an additional element so by adding this runnable Lambda to the chain from before I now have a chain that will return the length of the summary instead of the summary itself and here I can also check the type of the last element of the chain that I just added and it's of type runnable Lambda now we don't actually need to wrap the function in a runnable Lambda we can just add the function to the chain and then langin will make sure that the function is automatically converted to a runnable Lambda if I print the type of chain with function you can see that the last step of this chain is of runnable Lambda even though we didn't explicitly wrap the function in a runnable Lambda and let me just demonstrate that this chain actually works so I'm just going to invoke chain with function with the same context and here we get the summary length let's have a look at runnable pass through and first we'll go through the case where we use it as a placeholder I'm just going to instantiate a pass through and then I'm going to injected into the chain just before the length Lambda function and and as you can see we get the same output as before I can also check the types of the steps and you can see that the last step is still a runnable Lambda and the step before that is a runnable pass through passing data through without changing it like this is typically used with runnable parallel we can also use a runnable pass through for assigning value to the state of the chain to do that you will use the assign method of the runnable pass through and here I'm going to add the length of the summary to the state of the chain if I then add the assigned pass through as an additional component to the chain the output of the chain will now have an additional key with the length of the summary assigning an additional key value like this is particularly useful when you're working with SQL chains in some of my earlier videos on building custom SQL chains I'm actually using the assign method like this if I run this you can see that we now have the summary and the length and if I check the type of the last component of the chain I now get the type runnable assign the last example I'm going to give is the example of a runable parallel and in this case I will simply take the summary that the summarized chain is returning and then I'm going to Branch Out The Chain in two different arms one is going to take the summary and return the summary as it is this in the other one is going to return the length of the summary and this is going to give me something that is similar to what we got in the previous example so here my output contains the summary and the length of the summary and if I check the type of the last step of the chain I get a runnable parallel now let's talk about splits and retrievers I'm going to walk you through what a retriever is how we can split data into chunks and how we can load those chunks into a vctor store and in the cab notebook I'm going to give you a Hands-On example of working with a vector store I'm going to show you how you can use redis to build a vector store backed Retriever and do retrieval from Lang chain but let's start with answering the question what is a retriever a retriever in Lang chain is an interface case that returns documents given a query and this is not to be confused with a vector store so a vector store can be used as the backbone of a retriever but a retriever does not store the documents itself Vector store back retrievers are by far the most common retrievers when we build llm applications and when we build rag applications retrieval augmented generation applications we typically want to chunk up our data or split our data with a text splitter and store that in the vector store and langin has several built-in text Splitters that can be used for various different purposes it has a recursive text splitter that will try to keep the related chunks of text next to each other it has splitters for HTML markdown different types of code characters and tokens and I'll show you in a bit how to use the recursive text splitter when we get to the notebook so when the data has been chunked up into pieces we loaded into a vector store and then we can use that Vector store as the bases for a retriever that can be used in Chains or agents and if you go back and watch some of my earlier videos there are plenty of examples of using reddis and there's also a few examples of using pine cone but there are many different Vector databases to choose from and it's easy to switch up the vector database if you're using Lang chain because the interface is almost identical for for all the databases all right so let's head over to the collab notebook and see how we can work with Vector stores and how to use Splitters and retrievers so in this example I'm going to be using reddis as the vector store so I've pip installed redis and then I'm going to use the YouTube loader that we used before to extract the transcript from one of my videos here the loader will return a list of documents containing one document with the entire transcript of my video and now I can spit the text in the document using the recursive character text spitter and this text splitter is imported from langin text spitters I can then call the split documents method on the lists of documents containing a single document and then it will return a list of documents with chunks of text I could also have split the raw text by using the split text method of the text splitter once I have the chunks of text I want to load to a vector store I can connect to reddish or any other Vector database that Lang chain supports I'm going to need the host port and password in order to connect to redish and then the full reddish URL which is a combination of the three if you want to connect to redus through Lang chain and we want to do that here to use a hosted version of Reddit like I'm doing here just head over to reddit.com and sign up there a free tier and it's really easy to get started if you want to connect to reddis outside of L chain you can use reddish Pi so here I'm going to import reddis and then I'm connecting to reddis directly through redis py using the host port and password and once the connection has been established I can ping it to see that I actually have a connection and then I'm just going to flush the database to make sure it's empty next next up we're going to need some embeddings and I'm going to use hugging face embeddings since they are free so you'll need to pip install sentence Transformers I already did that here and then you can import hugging face embeddings from Lang chain embeddings and instantiate the embeddings it's going to take a few seconds to import this and when you're done you are ready to load the data to reddis I'm going to import Lang Chain's interface to redus because I want to load the vector data with Lang chain but I could also have done this manually using R pi and again I have an entire video covering that so just head over to the YouTube channel it should be easy to find so to load the vector data to radus we'll use the from documents method that's built into Lang chain Rus interface and as arguments we're going to pass in the list of documents with the chunks of text the embeddings the hugging phas embeddings and the reddish URL and then we're going to give it an index name let's call it YouTube and under the hood Lang chain is using the hogging face embeddings to create numeric vectors out of the text chunks and then load that to the vector database and once we have the data in the vector store we can set up the Retriever and again we use Lang chains radius object and we just call the method s Retriever with a search type and some keyword arguments and now we actually set up to do Vector similarity search over the documents in redish and just like we do with chains if we want to call the retriever we invoke it with a quiry and here we get all the chunks of data that are related in some way to data analysis and this list will contain exactly 10 documents as that's the parameter I set in the search keyword arguments so now we have a retriever and from here it's pretty easy to build a rack chain now let's move on to Rack retrieval augmented generation what is rack so rack is a technique for augmenting at l m knowledge with external data and a rag application is built in two steps the first one is indexing the external data that's what we did in the last section and in the second step we retrieve and generate the output with the llm so you already saw how the first step works we take some documents we chunk them up we turn them into numeric vectors and then we Lo them to a Victor database like reddish and the second step we retrieve and generate so the user query is used to fetch the context from the vector store and then that context is sent to the llm along side with the query itself and this gives the llm what it needs to generate a contextual output in a moment I'm going to show you how to build a rack chain and Lang chain using the retriever we just set up in the last section but before I do that I quickly want to make a comment on something I've noticed that a lot of AI practitioners have been thinking about do we really need to worry about this can we abstract away retrieval when we build chat Bots and rag applications if you're familiar with the OM assistance API you'll know that a lot of this is abstracted away when you build assistance that will do retrieval from a file and on the surface it looks like openi just removed a huge headache and made it a lot easier to build rag applications without having to worry about building retrievers but there is nothing simple about serving the right context to an llm based chatbot so if you're trying to build a chatbot that will convert customers for instance you're really facing a search problem and you generally have two metrics that you want to optimize precision and recall recall measures the completeness of the search results so how many of the relevant documents are we actually fetching and stuffing into the context window and precision measures the quality of the batch we are retrieving so the density of the relevant search results and in order to optimize the conversion rates in the chatbot you need both of these metrics and to do that you need control over the retrieval process which you don't have if you abstract away the retriever so just think about how many businesses are currently doing C conversion rate optimization on web pages and as more and more businesses are implementing chatbots and chat-based interfaces a lot of the C work is going to be focused on optimizing retrievers so I wouldn't bet on abstracting away retrievers at least not in the long run now let's build a rag chain using our reddish based retriever for this example I'm going to use a chat promp template and I'm injecting a context and the users's question into the template then I'm going to use a string output passer that allows me to extract the text output from the llm to set up the chain I'm going to use Lang chain expression language and in the first I'm extracting the question from the input data using the Lambda function and then passing the question to the retriever to get the context so the context here is a composition of two elements and then I also need to pass the question itself to the LM and I do that in the second line and then I use the pipe operator to assemble the entire chain consisting of the first dictionary The Prompt the llm and the string output passer and that's it now we have a rag chain we can then invoke the chain to get an answer to the question what can we do with llama 3 and this answer will then be based on the context which is given by the documents fetched by the retriever moving on to tools so what is a tool what is a toolkit and how do we use a tool tools and Lang chain are interfaces that agents chains or llms can use to interact with the world and I think a good way to think about tools is that just like humans need tools to perform tasks llms need tools as well for instance if you want to do some complex calculation that you can't do in your head you would need a tool for that a calculator or a computer program and similarly for the llm to be able to perform a task it will need access to computational resources or additional data in Lang chain you have several built-in tools you have the python Rebel tool you have a Wikipedia Tool YouTube Tool Saia tool and gradi Tool and so on and more and more tools are constantly being developed Lang chain also has built-in tool kits that are essentially collections of tools that are designed to be used to solve a specific task and examples of toolkits includes the air toolkit GitHub toolkit G lab toolkit and so on and just like tools the number of tool kits available for Lang chain is ever increasing so how do we use a tool and there are essentially two direct ways we can use a tool one is that we can directly run the tool with a query and just like loaders this will typically return some data or we can bind the tools to an llm and then we get an llm with access to tools that we can use in Chains and agents now let me show you how to do that by building on our YouTube example here in the notebook I'm going to pip install YouTube search and then I'm going to import YouTube search tool from Lang chain Community tools and the YouTube search tool allows me to search for YouTube videos so I'll in State the tool and then I can run the tool with the query rabid metric and as you can see this is going to return a list of videos which are videos from my YouTube channel now I can take this tool and I can bind it to the llm and then I get an llm with tools and now I can invoke the llm with tools with a query if I do that you can see that I get an AI message with some search arguments I can extract tool calls and that will give me a clear overview of the arguments and these are the arguments that you need to pass to the YouTube Tool now I can use a Lambda function to extract the arguments and then I can compose a chain consisting of my llm with tools the Lambda function and my YouTube Tool and this allows me to invoke the chain with random queres about YouTube videos so if I ask find some rabid Metric videos on Lang chain you can see that the chain is now returning me the list of YouTube videos so this is not much different than just running the search but there is now some llm based reasoning behind the crew and this is going to be more interesting once we start building agents finally let's talk about agents I'm going to to explain what an agent is and then we're going to build a YouTube summary agent what are agents so agents are programs capable of reasoning and Performing tasks by integrating llms with tools and they do so by leveraging the lm's capability to generate Json or other structured output this would be a very general definition of the concept in Lang chain agents have a very specific meaning in Lang chain an agent is a class that uses an llm to choose a sequence of actions to take the main conceptual difference between an agent and a chain is that in a chain the sequence of actions are hardcoded whereas in an agent we let the LM decide on what actions to take and what tools to use so think of the chain as a fixed sequence of steps whereas the agent might loop back and use the llm to decide on using a new tool and take a new action and this will go on until the llm decides that the task is completed in theory the agents are more flexible and robust than the chains however we also know that LMS can hallucinate so it depends on the use case now let's build an agent out of our YouTube use case after pip installed Lang chain hop I'm going to use that to extract a pre-made prompt and I'm going to import hop from Lang chain I'm going to import agent executor from Lang chain agents and create tool calling agent from Lang chain agents and the pre-made prom I'm going to use is available on the Hub and there you can see the template that's being used and then there's a variable Called Agent scratch pad and the agent scratch Pad is an important variable it is where the agent will save its intermediary steps but this prompt is ready to use along with our YouTube Tool and the llm and these are the only three things we need to create the agent so I'm going to pass the llm the tools the prompt to create tool calling agent and then I'll set up the agent executor and just like we did with the chains I can now invoke the agent executor and I'm going to ask the agent to find some Lang chain YouTube videos and here we see the list that it extracts with the tool and then it ranks the videos and finishes the chain and if we try to click on the first video we see that it gets Lang chain expained in 13 minutes by rabid metric what a brilliant agent now the cool thing about an agent is that we can just give it access to multiple tools and it will figure out on its own how to use the tools I'm now going to create a custom transcribe video tool using the tool decorator and the YouTube loader we used in section two and this tool will take a video URL and return the transcript of the video as a document then I'm going to create a list of tools with two tools the YouTube search tool which just used and the transcribe video tool and of course you need to have both YouTube search and the YouTube transcript API pip installed to do this and now we can invoke the agent executor and ask what topics the rabbit metric YouTube channel covers the YouTube agent will now use both tools add its disposal to give me a summary of the topics covered on the channel and here we have the final output with the summary of the topics covered all right that's it for now if you want to dive deeper into Lang chain I suggest you go check out some of the more advanced tutorials on the channel and if you enjoyed this tutorial and want to follow along for more content like this give it a like And subscribe thanks for watching\")]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38303fa6-93e5-4fa6-a2c3-7af1e998aaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a75c47ac-f49b-4b98-a5ae-1551c5a06b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a helpful assistant that explains YT videos. Given the following video transcript:\n",
    "{video_transcript}\n",
    "give a summary.\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=['video_transcript'],\n",
    "    template = prompt_template,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "322445b0-182b-4da2-84fa-f358c385262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm\n",
    "summary = chain.invoke({'video_transcript': docs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64dea504-c828-42bd-932c-25d1165eb0d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='In this tutorial, we learned about Lang chain, a tool for building AI agents using natural language processing (NLP) and machine learning (ML). We covered the following topics:\\n\\n1. Introduction to Lang chain: Lang chain is a tool that allows us to build AI agents by integrating language models (LM) with tools and chains. It provides a way to compose LM-based reasoning with tool-based actions.\\n2. Agents vs Chains: Agents are programs that use an LM to choose a sequence of actions to take, while chains are a fixed sequence of steps. Agents are more flexible and robust than chains, but they also have the potential to hallucinate.\\n3. Building an agent: We built an agent using a pre-made prompt and our YouTube Tool. The agent used the YouTube Tool to search for videos on Lang chain and then ranked the videos based on the rabid metric.\\n4. Custom tools: We created a custom transcribe video tool using the tool decorator and the YouTube loader. This tool takes a video URL and returns the transcript of the video as a document.\\n5. Invincing the agent executor: We invoked the agent executor and asked the agent to find some Lang chain YouTube videos. The YouTube agent used both tools to give us a summary of the topics covered on the channel.\\n\\nOverall, Lang chain provides a powerful way to build AI agents that can reason and perform tasks by integrating LMs with tools and chains. By creating custom tools and invoking the agent executor, we were able to build an agent that could search for videos on Lang chain and give us a summary of the topics covered on the channel.', additional_kwargs={}, response_metadata={'model': 'llama2:latest', 'created_at': '2025-02-23T12:07:44.435904Z', 'done': True, 'done_reason': 'stop', 'total_duration': 88983775500, 'load_duration': 18844600, 'prompt_eval_count': 2048, 'prompt_eval_duration': 5341000000, 'eval_count': 361, 'eval_duration': 83615000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-2a91d046-dd3f-4c12-81b2-ed950a05a6e0-0', usage_metadata={'input_tokens': 2048, 'output_tokens': 361, 'total_tokens': 2409})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd9844c-c82a-49dd-89be-dee403f045ea",
   "metadata": {},
   "source": [
    "### LCEL AND RUNNABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "383c26c9-a9ce-4a46-96c3-a0a70fbce210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "summarize_prompt_template = \"\"\"\n",
    "you are helpful assisnat that summarizes AI concepts:\n",
    "{context}\n",
    "summaruze the context\n",
    "\"\"\"\n",
    "summarize_prompt = PromptTemplate.from_template(summarize_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0875f8e-0031-4a10-92c0-318a35e98c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nyou are helpful assisnat that summarizes AI concepts:\\n{context}\\nsummaruze the context\\n')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb693c74-e598-4e88-8200-fc1b8bcba8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = StrOutputParser()\n",
    "chain = summarize_prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc9b5fae-add5-488a-9b5d-93961974de54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain is a term used in the field of natural language processing (NLP) and machine learning (ML). It refers to a type of language model that is designed to process and generate text at scale. The term \"langchain\" is a combination of \"language\" and \"chain,\" with the latter referring to the sequential nature of language processing.\\n\\nThe context of LangChain can be understood in the following ways:\\n\\n1. Language Modeling: LangChain is a type of language model that is trained on large datasets of text to predict the next word or character in a sequence of text given the context of the previous words. This task is at the core of many NLP applications, such as language translation, text summarization, and chatbots.\\n2. Sequence-to-Sequence Learning: LangChain is also used in sequence-to-sequence learning tasks, where the goal is to generate a target sequence of text given an input sequence. This can be useful in applications such as machine translation, where the model needs to generate the translated text based on the input text.\\n3. Long-Form Text Generation: LangChain can also be used for long-form text generation tasks, such as generating articles, blog posts, or even entire books. The model is trained on a large dataset of text and can generate new text that is coherent and contextually relevant.\\n4. Improving Language Understanding: LangChain can also be used to improve language understanding by learning the relationships between words and phrases in a given language. This can be useful in applications such as sentiment analysis, where the model needs to understand the meaning of text in order to classify it correctly.\\n5. Generating Creative Content: Finally, LangChain can be used to generate creative content, such as poetry or short stories. The model is trained on a large dataset of text and can generate new text that is coherent and contextually relevant.\\n\\nIn summary, LangChain is a type of language model that is designed to process and generate text at scale. It can be used for a variety of NLP tasks, including language modeling, sequence-to-sequence learning, long-form text generation, improving language understanding, and generating creative content.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"context\": \"what is Langchain\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "217001ce-cf70-480b-a035-a02503881a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8462a5b1-d5dc-4860-8942-4bba127a3219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LangChain is a term used in the field of Natural Language Processing (NLP) and Artificial Intelligence (AI). It refers to a sequence of language models or \"langs\" that are connected in a chain-like structure. Each lang in the chain is trained on a different task or dataset, and the outputs from each lang are used as inputs for the next lang in the chain.\\n\\nThe context of LangChain can be summarized as follows:\\n\\n* LangChain is a type of neural network architecture that combines multiple language models (langs) into a single model.\\n* Each lang in the chain is trained on a different task or dataset, such as text classification, sentiment analysis, or machine translation.\\n* The outputs from each lang are used as inputs for the next lang in the chain, allowing the model to learn complex relationships between different language tasks.\\n* LangChain can be seen as a way to leverage the strengths of multiple models and create a more powerful and flexible AI system.\\n\\nSome potential applications of LangChain include:\\n\\n* Improving machine translation by using a chain of langs trained on different languages to generate more accurate translations.\\n* Enhancing text classification by combining the outputs of langs trained on different sub-tasks, such as sentiment analysis and named entity recognition.\\n* Building a conversational AI system that can handle multiple tasks and adapt to new inputs by chaining together langs trained on different domains.\\n\\nOverall, LangChain is a promising approach to combining multiple language models into a single model, with potential applications in a wide range of NLP tasks.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize_chain = summarize_prompt | llm | output_parser\n",
    "summarize_chain.invoke({\"context\": 'what is langchain'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8550be23-9946-45d3-b5f8-38ae61ecff42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'LangChain is a term used in the field of natural language processing (NLP) and machine learning (ML). It refers to a type of language model that uses a chain of neural networks to process and generate text. The term \"langchain\" is a combination of \"language\" and \"chain,\" reflecting the way these models are designed to process sequences of words or characters in a chain-like manner.\\n\\nThe context of LangChain can be summarized as follows:\\n\\n* LangChain is a type of language model that uses a sequence of neural networks to process and generate text.\\n* Each neural network in the chain processes a portion of the input text, with earlier networks processing more basic features such as individual words or characters, and later networks processing more complex features such as sentences or paragraphs.\\n* The output of each network is passed on to the next network in the chain, allowing the model to capture longer-range dependencies and contextual relationships between elements in the input text.\\n* LangChains are often used for tasks such as language translation, text summarization, and language generation, where they can take advantage of their ability to process long sequences of text and capture complex contextual relationships.\\n* Compared to other types of language models, such as recurrent neural networks (RNNs) or transformer-based models, LangChains have the advantage of being more parallelizable, allowing for faster training times and more efficient use of computational resources.\\n\\nOverall, LangChain is a type of language model that leverages the strengths of both RNNs and transformer-based models, while also addressing some of their limitations. By processing text in a chain-like manner, LangChains can capture complex contextual relationships and generate more accurate and natural-sounding output.',\n",
       " 'length': 'Summary length: 1826 characters'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length_lambda = RunnableLambda(lambda summary : {\n",
    "    'summary': summary,\n",
    "    'length' : f'Summary length: {len(summary)} characters'})\n",
    "lambda_chain = summarize_chain | length_lambda\n",
    "lambda_chain.invoke({'context': \"What is Langchain\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "eb031190-4bed-4c27-8325-db96987bb18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'LangChain is a term used in the field of Natural Language Processing (NLP) and Machine Learning (ML) to refer to a type of language model. Specifically, LangChain models are a class of sequence-to-sequence models that are designed to handle long-range dependencies in sequential data, such as text.\\n\\nThe term \"LangChain\" is derived from the words \"language\" and \"chain,\" and it refers to the way these models process sequences of tokens (such as words or characters) by chaining together multiple layers of processing steps. Each layer in a LangChain model processes the input sequence one token at a time, using the output of the previous layer as input to the next layer. This allows the model to capture complex contextual relationships between tokens in the input sequence.\\n\\nLangChains are particularly useful for tasks such as language translation, text summarization, and language generation, where the model needs to be able to handle long-range dependencies in order to produce coherent and fluent output. They have been shown to achieve state-of-the-art results on a number of NLP tasks, including machine translation and text summarization.\\n\\nIn summary, LangChain is a type of language model that processes sequences of tokens by chaining together multiple layers of processing steps, allowing it to capture complex contextual relationships between tokens in the input sequence.',\n",
       " 'length': 'Summary length: 1388 characters'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "summarize_chain = summarize_prompt| llm | output_parser\n",
    "passthrough = RunnablePassthrough()\n",
    "placeholder_chain = summarize_chain | passthrough | length_lambda\n",
    "placeholder_chain.invoke({'context' : 'what is langchain'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbe23d50-8f6c-4bc5-82c9-b57fce58652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap_summary_lambda = RunnableLambda(lambda summary : {'summary' : summary})\n",
    "assign_passthrough = RunnablePassthrough.assign(length = lambda x : len(x['summary']))\n",
    "summarize_chain = summarize_prompt | llm | output_parser | wrap_summary_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e6e6514-616f-4e28-a810-f6cf44b5cb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': \"LangChain is a term used in the field of Natural Language Processing (NLP) and Artificial Intelligence (AI). It refers to a sequence of language models or transformers that are trained on each other's outputs, with the goal of improving the performance of the final model.\\n\\nThe idea behind LangChain is to leverage the strengths of multiple models to achieve better results than any single model could produce alone. By training one model on the output of another model, the subsequent models in the chain can learn from the errors or limitations of the previous models and improve their own performance. This can lead to a significant increase in accuracy and robustness compared to a single model trained directly on the task at hand.\\n\\nLangChain is particularly useful when dealing with complex tasks that require a deep understanding of language, such as text classification, sentiment analysis, and machine translation. By training multiple models in a chain, each one can specialize in a different aspect of the task, leading to better overall performance.\\n\\nThe context of LangChain typically involves training a series of transformer-based models on a sequence of tasks, with each subsequent model building on the output of the previous one. For example, a LangChain might involve training a language model on a set of text classification tasks, followed by a sentiment analysis task, and then a machine translation task. Each model in the chain can learn from the errors or limitations of the previous models, leading to improved performance at each stage.\\n\\nOverall, LangChain is a powerful technique for improving the performance of NLP models by leveraging the strengths of multiple models trained on each other's outputs.\",\n",
       " 'length': 1731}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assign_chain = summarize_chain | assign_passthrough\n",
    "assign_chain.invoke({'context': 'what is langchain'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a017198d-fc93-433e-a0bf-cc809a680356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'Langchain is a term used in the field of natural language processing (NLP) and machine learning (ML). It refers to a type of language model that is trained on a large corpus of text data, and is designed to generate coherent and fluent text.\\n\\nThe term \"langchain\" is derived from the words \"language\" and \"chain,\" and was coined by researchers in the field of NLP to describe this type of language model. The idea behind a langchain is to create a chain of language models, each one building on the output of the previous one, to generate text that is more coherent and natural-sounding than would be possible with a single language model.\\n\\nThe context of langchains is in the field of NLP, where researchers are constantly seeking ways to improve the quality and fluency of machine-generated text. Langchains are one approach to achieving this goal, by allowing multiple language models to work together to generate more coherent and natural-sounding text.\\n\\nSome key aspects of langchains include:\\n\\n1. Chain structure: A langchain is composed of a series of language models, each one connected to the next through a chain structure. Each model in the chain takes the output of the previous model as input, and generates a new output based on that input.\\n2. Training data: Langchains are typically trained on large corpora of text data, such as books, articles, or websites. The training data is used to train each language model in the chain, so that they can learn the patterns and structures of natural language.\\n3. Coherence: One of the main goals of a langchain is to generate coherent and fluent text. To achieve this, the models in the chain are designed to work together to create a continuous and logical flow of language.\\n4. Flexibility: Langchains can be trained on a wide range of text data, including different genres, styles, and topics. This makes them flexible and adaptable to a variety of applications, such as chatbots, language translation, or content generation.\\n\\nOverall, langchains are a promising approach to improving the quality and fluency of machine-generated text, and have the potential to enable new applications and use cases in NLP.',\n",
       " 'length': 2165}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "summarize_chain = summarize_prompt | llm | output_parser\n",
    "parallel_runnable = RunnableParallel(\n",
    "    summary = lambda x : x,\n",
    "    length = lambda x: len(x)\n",
    ")\n",
    "parallel_chain = summarize_chain | parallel_runnable\n",
    "parallel_chain.invoke({'context' : \"what is Langchain\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249f9d24-1c81-4e15-96f1-6468c7f0b269",
   "metadata": {},
   "source": [
    "## SPLITTERS AND RETRIEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e80205f9-ec99-4097-8037-ef91a4d9f6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade --quiet redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67a9c36f-e905-402b-a42d-522c0891bdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3cc22ba-c490-43e2-b538-d811c1743915",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = YoutubeLoader.from_youtube_url(\"https://youtu.be/8BV9TW490nQ?si=lFVcy5SuQ5dBueU5\", add_video_info = False)\n",
    "doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35a8d28f-3532-47a4-a2b2-f3248845152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap = 20,\n",
    "    length_function = len,\n",
    "    is_separator_regex = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c7f8461-81e9-4782-baa0-5c6085c0e95d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"in this video you'll learn everything you need to know about Lang chain in seven easy steps I've\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"easy steps I've broken down the framework into key components and here's an overview of the image\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"of the image I'm going to try to put in your mind in the next 40 minutes there is a lot to unpack\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"is a lot to unpack but I'll walk you through the components one by one and I've included coding\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"included coding examples with every step I'll start off by asking why length chain and I'll give\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"chain and I'll give you a reason why you want to learn this and an overview of the framework then\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the framework then I'll show you how to build chains using prompts and ERS and we'll dive into the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"we'll dive into the runable protocol and Lang chain expression language then we'll look at\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"then we'll look at Splitters and retrievers and we'll build a rag chain finally I'll walk you\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"I'll walk you through how to use tools and how to build an agent with access to Tools in this video\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"Tools in this video I'll keep my focus on the core Lang chain framework and build the foundation\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the foundation for advanced use cases so I'll save Lang graph Lang Smith and Lang surf for separate\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"surf for separate videos if you're new to Lang chain this tutorial will serve as a quick start and\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"a quick start and if you're already already familiar with the framework you can use the tutorial to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"use the tutorial to tie up some loose ends I'll share a link to this interactive map of Lang chain\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"map of Lang chain to the code and to a written tutorial below this video let's get started I'm\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"get started I'm going to start off by giving you an overview of the linkchain framework what is it\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"what is it why would you want to use it and then I'm going to show you how to get started\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='how to get started developing in a collab notebook so what is Lang chain Lang chain is an open open'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='is an open open source framework for developing llm applications an engine allows developers to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='developers to combine llms with external sources of computation and data and the framework further'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='framework further simplifies the deployment of the llm applications by providing the needed tools'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the needed tools such as tools for inspection monitoring and evaluation the deployment and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the deployment and monitoring tools are part of Lang Smith and Lang serf I'm not going to cover\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"not going to cover that here I'm going to cover the core Library so Lang chain combines llms with\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='combines llms with external sources of computation and data how does that work suppose that a user'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='suppose that a user wants to ask a question related to some documents or some data that the user'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"data that the user has access to now this question will be sent to the llm just like when you're\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"like when you're interacting with chat DBT but you also need to fetch the context needed to answer\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='needed to answer the question from your own data or your own documents and to do that a vector'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to do that a vector representation of the initial question from the user is used to fetch all the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to fetch all the relevant context needed to answer the question typically from a vector database'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='a vector database this context is then fed to the llm alongside with the initial question and now'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='question and now the llm has everything it needs to answer the question in the right context and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='right context and the magical thing about this is that we can do more than just answer the question'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='answer the question we can actually take actions based on the input from the end user and we can do'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='user and we can do that because Lang chain allows us to build agents out of llms and agents are'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='llms and agents are created by by combining llms with tools this will give the llms access to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the llms access to external sources of computation and data so what can we build with Lang chain'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='with Lang chain when it comes to the types of applications you can build with llms this guise the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='llms this guise the limit from what I see most llm applications that have been built over the last'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='built over the last year fall into one or three categories chatbots racun applications or agent'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"or agent systems so the chat BS are nothing new they're used for marketing customer support\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='customer support different types of education and creating assistance what is new is that these'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='new is that these chat Bots can now be made more intelligent using llms then we have the rag Q&A'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='we have the rag Q&A applications and this is the magic that Lang chain and llama index made people'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='index made people aware of a year ago so these are the applications that are used for summarizing'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='for summarizing large documents doing data analysis and generating code by referencing repositories'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='repositories and then we have what we can call agent system both multi-agent systems and systems'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='systems and systems where we Loop humans into complex interactions with agents and this is what'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and this is what Lang graph is used for and Lang graph is a rather new component in the Lang chain'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='in the Lang chain ecosystem the number of potential applications of agent systems is virtually'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"is virtually Limitless they're being implemented for Supply Chain management Revenue optimization\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"optimization operations management and so on all right so let's zoom out a bit and have a look at\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and have a look at why you want to learn langing so so there are at least two good reasons for'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='good reasons for learning Lang chain so first of all Lang chain facilitates the transitioning to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='transitioning to the next generation of programming languages and what do I mean by that so if we'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='by that so if we have a look at programming language hierarchies this is what it typically looks'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='it typically looks like so at the bottom of the pyramid you will have machine language on top of'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='language on top of that you will have Assembly Language then you have the higher level languages'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='level languages those are the languages that you are used to program coming in so python Java SQL R'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"python Java SQL R C++ and so on and we've been moving up the pyramid ever since programming became\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='programming became a thing and right now we are at the final step the highest level of all'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"level of all languages which is natural language because with the llms we don't have to write code\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='have to write code anymore we can have the llms write code for us by giving natural language as an'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='language as an input and the development of these pipelines that allows us to turn natural language'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='natural language into programs is exactly what Lang chain facilitates so we can create pipelines or'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='create pipelines or chains as they are called that allows us to input natural language and get'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='language and get something of value in return like a report an analysis or computer program and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='program and with the current generation of best-in-class llms we can already write executable code'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"executable code code that works so this is not something that belongs to the Future it's already\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"Future it's already here now the second reason you you might want to use Lang chain is really not\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"chain is really not Lang chain itself but what's being built on top of Lang chain which is Lang\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='chain which is Lang graph now Lang chain was late to the multi-agent game you had autogen you had'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='had autogen you had crew AI before that but what langra can do is really remarkable for instance'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='for instance Lang graph allows us to build complex interactions between agents and humans and this'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and humans and this is needed especially in big organizations where you have to guard the access to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='guard the access to certain files and so on but Lang graph also allows us to build multi-agent'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='build multi-agent systems like autogen or crew II and the use cases for this are becoming more'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='are becoming more clear every day we need to be able to set up arbitrary communication patterns'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"patterns between agents we also need to do divide and conquer just like humans we can't have an llm\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"can't have an llm do everything so we want to build specialized agents and we want them to be able\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"them to be able to perform tasks in an asynchronous way all right let's get started developing the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='developing the first thing you want to do is you want to fetch the API key from the llm service you'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the llm service you want to use so in this tutorial I'm going to use openi and anthropic but Lang\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='anthropic but Lang chain supports a wide array of llm providers so you can just choose your'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='just choose your favorite one once you have grabbed your API key you want to put it in a EnV file'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='it in a EnV file all right so here on the collab notebook I have already installed the needed'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the needed libraries here I'm going to install python. EnV to fetch API keys from the EnV file I'm\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the EnV file I'm going to install Lang chain of course Lang chain Community Omi and anthropic and I\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and anthropic and I have uploaded the EnV file from my local machine to the cab notebook and now'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"notebook and now I'm ready to load the environment variables so I'm loading the API keys and once\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"API keys and once I've done that I can use chat anthropic and chat Omi to connect to anthropic and\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"to anthropic and Omi and then I'm going to make a test call to GPT 4 and ask what is Lang chain and\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='is Lang chain and I simply make an LM call by using the invoke method and here we have an answer'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='we have an answer from gbt 4 this is very similar to what you would do if you were just calling gbt'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='just calling gbt 4 directly through the open API and of course we can just switch out the llm now'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"out the llm now I'm calling clot 3 and here I also get a response so I can also easily prpt the llm\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"easily prpt the llm with a system message and a user message at the same time and I'll just send\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"and I'll just send the two messages in a list to the llm and this will allow you to do some basic\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to do some basic prompt engineering but we can do a lot more advanced stuff than this once we start'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"this once we start utilizing prom templates and building chains all right so let's dive into chains\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"dive into chains prompts and loaders and build our first chain I'm going to walk you through what a\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"you through what a chain is and give you an analogy to think of then I'm going to work you through\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to work you through prom templates and how to use document loaders so a chain in Lang chain is a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"in Lang chain is a sequence of interconnected components that process a user's query utilizing one\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"query utilizing one or more llms to generate and deliver some valuable output and I'm going to get\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"I'm going to get into what those components are in a bit but let me just give you examples of the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='you examples of the valuable output that we can generate with the chain so examples of valuable'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of valuable output could be an answer to a specific question it could be a summary of some document'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of some document it could be a recommendation insights from data a piece of executable code or some'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='code or some Json for an API call and with the chains we can automate the generation of this value'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of this value now what about the components the components of a chain are typically proms llms or'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='proms llms or what we call chat models output passes that passes the output of the llm tools that'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the llm tools that allow the llms to extract additional information from apis or run some code this'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='run some code this is what turns llms into agents and general functions and in Lang chain these'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='in Lang chain these components are so-called runnables that can be chained together to produce a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to produce a pipeline and this is one way to think of chains as pipelines in the world of llms so'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='world of llms so an analogy would be a data pipeline in a data pipeline we typically input some raw'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='input some raw data and we get some nice clean data out in a table format and the data cleaning and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='data cleaning and transformation happening in the pipeline is valuable similarly in an llm chain we'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='in an llm chain we input a query and we get some valuable output in return and here the query is'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='here the query is turned into value using llm calls functions and additional data and just like'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='data and just like with data pipelines we can automate this process with cicd continuous'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='cicd continuous integration and continuous delivery and this is what Lang chain as a framework can'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='as a framework can help you do so not only can you build the chains but you can also test the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='can also test the chains deploy them monitor the chains and evaluate the output and this can be'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and this can be done with Lang Smith and Lang surf so Lang chain has several built-in modules or'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='built-in modules or classes that helps you create chains efficiently and one of those modules are'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='those modules are prompt templates so a prompt for an llm is a set of instructions provided by a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"provided by a user to guide the model's response and promt templates in Lang chain are predefined\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='are predefined prompt recipes these recipes are very helpful when you want to create chains fast'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='create chains fast another feature of Lang chain is that you have so-called document loaders a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='document loaders a document is a piece of text and Associated metadata and the Lang chain document'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='Lang chain document loaders provide a load method for loading data as documents from a source into'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"from a source into a chain now let's head over to a collab notebook and set up a simple chain I'm\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"a simple chain I'm going to import prompt template from Lang chain prompts and then I'm going to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"then I'm going to formulate the prompt to the llm as a string with a placeholder in this case with\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='in this case with the variable topic and then I set up the prompt using the prompt template I just'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"template I just imported from Lang chain an input variable that I'm going to call topic and then\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='call topic and then the string the template itself and this is a simple example of a prompt that'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of a prompt that now allows me to comp OS a chain and I do that using the pipe operator so I can'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='operator so I can take the prompt and the language model LL mgbt 4 and assemble a chain using those'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='a chain using those two components and now I can invoke the chain with an input variable in this'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='variable in this case topic so this was a very simple example of assembling a chain using the pipe'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"using the pipe operator let's try to make this a bit more advanced I'm going to create a chain that\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"create a chain that will transcribe one of my YouTube videos so I'm going to pip install the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to pip install the YouTube transcript API and then I can use one of the loaders that is spilled'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"that is spilled into L chain the YouTube loader so I'll import YouTube loader from Lang chain\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='from Lang chain Community document loaders and then I can simply pass the URL to one of my videos'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to one of my videos or any video to the loader and have the loader extract the transcript from the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='transcript from the video now if we have a look at what the loader extracts you can see that we get'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='can see that we get a list of documents if you want the transcript you can simply extract the page'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='extract the page content from the documents or you can simply pass the documents to the chain'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to the chain directly so fetching the raw transcript would be as simple as just fetching the single'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"fetching the single document in this list and then extracting the page content all right so let's\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"all right so let's set up a chain that will summarize YouTube videos given a transcript so here I'm\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"so here I'm going to inject the video transcript into the prom template so this is going going to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='is going going to be my input variable and then I just set up the chain like I did before with the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"did before with the pipe operator and I'm going to invoke it with the video transcript and note\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"transcript and note that I can just pass the documents I don't have to extract the raw transcript\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the raw transcript and here we have a nice summary of my YouTube video so Lang chain also has a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='chain also has a number of build-in chains that you can use for stuff like this for instance the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='for instance the create stuff documents chain and this chain will take a list of documents and pass'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='documents and pass them all in as context in the prom template pretty much what we did before just'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='we did before just without using the buil-in chain this was just a very basic introduction to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='introduction to building chains now in order to build more advanced chains we need to understand'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='need to understand Lang chains runnable protocol and Lang chain expression language in more detail'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"in more detail let's now try to dive deeper into Lang chain expression language and the runnable\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and the runnable protocol the Lang chain expression language simplifies building complex chains'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='complex chains from basic components and we do that using the pipe operator that allows us to chain'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='allows us to chain different components together feeding the output from one component to the next'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to the next so a simple example of a chain composed in this way would be a prompt combined with a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='combined with a model and an output passer and these components are so-called runnables think of'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='runnables think of Lang chain expression language as a declarative way of composing these so-called'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"these so-called runnables into chains and I'll get into what a runnable is in a second here we have\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='second here we have a step-by-step example of what a chain looks like we have an input to the chain'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='input to the chain which is typically a dictionary that is sent to a prompt then a prompt value is'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='a prompt value is sent to the or the chat model then the chat model will return a chat message that'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='a chat message that we want to pass and we typically want to use the passer to extract a string'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to extract a string from the chat message and this will be the output of the chain this is one'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='chain this is one example of a basic chain and L chain allows us to chain multiple chains together'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='chains together like this so we might want to use the string output of the chain and send that to a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"and send that to a new chain that will return a different output so what are runnables let's try to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"let's try to understand the runnable protocol this definition is directly from Lang chain a\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='from Lang chain a runnable is a unit of work that can be invoked batched streamed transformed and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='transformed and composed and the chains we built with Lang chain and the components of those chains'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of those chains are runnables and apart from Lang chain native components being runnables we can'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='runnables we can actually also pass in random functions into chain and those functions will then'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"functions will then turn into runnables I'll show you that in an example when we get to the code\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='we get to the code and these chains can be invoked that means we execute the chain we can call'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='chain we can call batch on them if we have multiple inputs and we can stream the chains there are'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='chains there are some core runnable objects in Lang chain that you want to understand in order to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='in order to fully appreciate the Lang chain expression language these four objects are the most'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='are the most important ones runnable sequence runnable Lambda runnable pass through and runnable'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"and runnable parallel so let's start from the top a runnable sequence in Lang chain is a class that\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='is a class that chains together multiple runnable components and it ensures that each component'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='that each component processes its input and sequentially passes its output to the next component in'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"next component in general the change you're building in Lang chain will be of type runnable\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='be of type runnable sequence now what about runnable Lambda runnable Lambda is a class that turns a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='class that turns a python callable like a function into a runnable component and this allows us to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='this allows us to integrate arbitary functions into chains as in the example I just showed you then'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='showed you then we have runnable pass through and a runnable pass through in Lang chain is a class'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='chain is a class that either passes its input through unchanged or adds additional keys to the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='keys to the output in the first case it acts as a placeholder and in the second case it allows us'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='case it allows us to do flexible Integrations into sequences where we need to modify the input and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the input and finally we have runnable parallel and a runnable parallel in L chain is a class that'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='is a class that runs multiple runnables concurrently and this allows you to do branching where you'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"branching where you have two chains run the same input but return different outputs now let's see\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"now let's see how this works by looking at some code examples all right so I'm going to use the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"going to use the same symbol example throughout this section I'm going to have an llm summarize AI\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='an llm summarize AI Concepts here I have a prompt that tells the llm to summarize an AI concept and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"an AI concept and then I'm going to inject a context into the prompt the first thing I'm going to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"thing I'm going to do is I'm going to create a simple chain using the pipe operator we already did\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"we already did it once now I'm going to add an additional element the output passer so the chain is\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='so the chain is composed of three elements the summarized prompt the llm and the output passer and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='output passer and when we invoke the chain we get the text output now remember that chains are of'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='that chains are of type runnable sequence and we can check the type of the chain that we just'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"chain that we just created to see if that is the case so I'm just going to print the type of the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the type of the chain and here we can see that it is indeed of type runnable sequence now let's try\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"now let's try to use a runnable Lambda here we have the same summarized chain as above but now with\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='above but now with an additional element so I have a function that will calculate the length of the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the length of the summary and then I'm wrapping this function in a runable Lambda this allows me to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='this allows me to extend the chain we had before with an additional element so by adding this'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='so by adding this runnable Lambda to the chain from before I now have a chain that will return the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='will return the length of the summary instead of the summary itself and here I can also check the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"can also check the type of the last element of the chain that I just added and it's of type\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"and it's of type runnable Lambda now we don't actually need to wrap the function in a runnable\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='in a runnable Lambda we can just add the function to the chain and then langin will make sure that'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='will make sure that the function is automatically converted to a runnable Lambda if I print the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='if I print the type of chain with function you can see that the last step of this chain is of'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"of this chain is of runnable Lambda even though we didn't explicitly wrap the function in a\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the function in a runnable Lambda and let me just demonstrate that this chain actually works so I'm\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"works so I'm just going to invoke chain with function with the same context and here we get the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"and here we get the summary length let's have a look at runnable pass through and first we'll go\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"and first we'll go through the case where we use it as a placeholder I'm just going to instantiate\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"to instantiate a pass through and then I'm going to injected into the chain just before the length\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='before the length Lambda function and and as you can see we get the same output as before I can'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='as before I can also check the types of the steps and you can see that the last step is still a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='step is still a runnable Lambda and the step before that is a runnable pass through passing data'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='passing data through without changing it like this is typically used with runnable parallel we can'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='parallel we can also use a runnable pass through for assigning value to the state of the chain to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"of the chain to do that you will use the assign method of the runnable pass through and here I'm\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"and here I'm going to add the length of the summary to the state of the chain if I then add the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='if I then add the assigned pass through as an additional component to the chain the output of the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the output of the chain will now have an additional key with the length of the summary assigning an'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"assigning an additional key value like this is particularly useful when you're working with SQL\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"working with SQL chains in some of my earlier videos on building custom SQL chains I'm actually\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"chains I'm actually using the assign method like this if I run this you can see that we now have\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='that we now have the summary and the length and if I check the type of the last component of the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"component of the chain I now get the type runnable assign the last example I'm going to give is the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to give is the example of a runable parallel and in this case I will simply take the summary that'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the summary that the summarized chain is returning and then I'm going to Branch Out The Chain in\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='Out The Chain in two different arms one is going to take the summary and return the summary as it'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the summary as it is this in the other one is going to return the length of the summary and this is'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='summary and this is going to give me something that is similar to what we got in the previous'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='got in the previous example so here my output contains the summary and the length of the summary'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of the summary and if I check the type of the last step of the chain I get a runnable parallel now'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"parallel now let's talk about splits and retrievers I'm going to walk you through what a retriever\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='what a retriever is how we can split data into chunks and how we can load those chunks into a vctor'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"chunks into a vctor store and in the cab notebook I'm going to give you a Hands-On example of\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"Hands-On example of working with a vector store I'm going to show you how you can use redis to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"can use redis to build a vector store backed Retriever and do retrieval from Lang chain but let's\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"chain but let's start with answering the question what is a retriever a retriever in Lang chain is\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='in Lang chain is an interface case that returns documents given a query and this is not to be'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='this is not to be confused with a vector store so a vector store can be used as the backbone of a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the backbone of a retriever but a retriever does not store the documents itself Vector store back'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='Vector store back retrievers are by far the most common retrievers when we build llm applications'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='llm applications and when we build rag applications retrieval augmented generation applications we'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='applications we typically want to chunk up our data or split our data with a text splitter and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='a text splitter and store that in the vector store and langin has several built-in text Splitters'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='text Splitters that can be used for various different purposes it has a recursive text splitter'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='text splitter that will try to keep the related chunks of text next to each other it has splitters'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"it has splitters for HTML markdown different types of code characters and tokens and I'll show you\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"and I'll show you in a bit how to use the recursive text splitter when we get to the notebook so\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to the notebook so when the data has been chunked up into pieces we loaded into a vector store and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='a vector store and then we can use that Vector store as the bases for a retriever that can be used'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='that can be used in Chains or agents and if you go back and watch some of my earlier videos there'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"videos there are plenty of examples of using reddis and there's also a few examples of using pine\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"of using pine cone but there are many different Vector databases to choose from and it's easy to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"and it's easy to switch up the vector database if you're using Lang chain because the interface is\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the interface is almost identical for for all the databases all right so let's head over to the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='head over to the collab notebook and see how we can work with Vector stores and how to use'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"and how to use Splitters and retrievers so in this example I'm going to be using reddis as the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"using reddis as the vector store so I've pip installed redis and then I'm going to use the YouTube\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to use the YouTube loader that we used before to extract the transcript from one of my videos here'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of my videos here the loader will return a list of documents containing one document with the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='document with the entire transcript of my video and now I can spit the text in the document using'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the document using the recursive character text spitter and this text splitter is imported from'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='is imported from langin text spitters I can then call the split documents method on the lists of'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='on the lists of documents containing a single document and then it will return a list of documents'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='a list of documents with chunks of text I could also have split the raw text by using the split'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='by using the split text method of the text splitter once I have the chunks of text I want to load'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='text I want to load to a vector store I can connect to reddish or any other Vector database that'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"database that Lang chain supports I'm going to need the host port and password in order to connect\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='in order to connect to redish and then the full reddish URL which is a combination of the three if'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of the three if you want to connect to redus through Lang chain and we want to do that here to use'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"do that here to use a hosted version of Reddit like I'm doing here just head over to reddit.com and\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"to reddit.com and sign up there a free tier and it's really easy to get started if you want to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"if you want to connect to reddis outside of L chain you can use reddish Pi so here I'm going to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"here I'm going to import reddis and then I'm connecting to reddis directly through redis py using\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='redis py using the host port and password and once the connection has been established I can ping'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"I can ping it to see that I actually have a connection and then I'm just going to flush the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"going to flush the database to make sure it's empty next next up we're going to need some\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"going to need some embeddings and I'm going to use hugging face embeddings since they are free so\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"they are free so you'll need to pip install sentence Transformers I already did that here and then\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='that here and then you can import hugging face embeddings from Lang chain embeddings and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"embeddings and instantiate the embeddings it's going to take a few seconds to import this and when\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"this and when you're done you are ready to load the data to reddis I'm going to import Lang Chain's\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"import Lang Chain's interface to redus because I want to load the vector data with Lang chain but I\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='Lang chain but I could also have done this manually using R pi and again I have an entire video'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='an entire video covering that so just head over to the YouTube channel it should be easy to find so'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"be easy to find so to load the vector data to radus we'll use the from documents method that's\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"method that's built into Lang chain Rus interface and as arguments we're going to pass in the list\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to pass in the list of documents with the chunks of text the embeddings the hugging phas embeddings'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"phas embeddings and the reddish URL and then we're going to give it an index name let's call it\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"name let's call it YouTube and under the hood Lang chain is using the hogging face embeddings to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='face embeddings to create numeric vectors out of the text chunks and then load that to the vector'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='that to the vector database and once we have the data in the vector store we can set up the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='we can set up the Retriever and again we use Lang chains radius object and we just call the method'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='call the method s Retriever with a search type and some keyword arguments and now we actually set'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='now we actually set up to do Vector similarity search over the documents in redish and just like we'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and just like we do with chains if we want to call the retriever we invoke it with a quiry and here'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='a quiry and here we get all the chunks of data that are related in some way to data analysis and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"data analysis and this list will contain exactly 10 documents as that's the parameter I set in the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"I set in the search keyword arguments so now we have a retriever and from here it's pretty easy to\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"it's pretty easy to build a rack chain now let's move on to Rack retrieval augmented generation\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='generation what is rack so rack is a technique for augmenting at l m knowledge with external data'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='with external data and a rag application is built in two steps the first one is indexing the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"one is indexing the external data that's what we did in the last section and in the second step we\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the second step we retrieve and generate the output with the llm so you already saw how the first'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='saw how the first step works we take some documents we chunk them up we turn them into numeric'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='them into numeric vectors and then we Lo them to a Victor database like reddish and the second step'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and the second step we retrieve and generate so the user query is used to fetch the context from'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the context from the vector store and then that context is sent to the llm along side with the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='along side with the query itself and this gives the llm what it needs to generate a contextual'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"a contextual output in a moment I'm going to show you how to build a rack chain and Lang chain\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and Lang chain using the retriever we just set up in the last section but before I do that I'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"before I do that I quickly want to make a comment on something I've noticed that a lot of AI\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='that a lot of AI practitioners have been thinking about do we really need to worry about this can'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"about this can we abstract away retrieval when we build chat Bots and rag applications if you're\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"if you're familiar with the OM assistance API you'll know that a lot of this is abstracted away\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='is abstracted away when you build assistance that will do retrieval from a file and on the surface'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and on the surface it looks like openi just removed a huge headache and made it a lot easier to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='it a lot easier to build rag applications without having to worry about building retrievers but'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='retrievers but there is nothing simple about serving the right context to an llm based chatbot so'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"based chatbot so if you're trying to build a chatbot that will convert customers for instance\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"for instance you're really facing a search problem and you generally have two metrics that you want\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='that you want to optimize precision and recall recall measures the completeness of the search'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of the search results so how many of the relevant documents are we actually fetching and stuffing'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and stuffing into the context window and precision measures the quality of the batch we are'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of the batch we are retrieving so the density of the relevant search results and in order to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and in order to optimize the conversion rates in the chatbot you need both of these metrics and to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"metrics and to do that you need control over the retrieval process which you don't have if you\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"don't have if you abstract away the retriever so just think about how many businesses are currently\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='are currently doing C conversion rate optimization on web pages and as more and more businesses are'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='more businesses are implementing chatbots and chat-based interfaces a lot of the C work is going to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"C work is going to be focused on optimizing retrievers so I wouldn't bet on abstracting away\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"on abstracting away retrievers at least not in the long run now let's build a rag chain using our\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"rag chain using our reddish based retriever for this example I'm going to use a chat promp template\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"chat promp template and I'm injecting a context and the users's question into the template then I'm\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"template then I'm going to use a string output passer that allows me to extract the text output\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"the text output from the llm to set up the chain I'm going to use Lang chain expression language\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"expression language and in the first I'm extracting the question from the input data using the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='data using the Lambda function and then passing the question to the retriever to get the context so'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='get the context so the context here is a composition of two elements and then I also need to pass'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='I also need to pass the question itself to the LM and I do that in the second line and then I use'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='line and then I use the pipe operator to assemble the entire chain consisting of the first'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"of the first dictionary The Prompt the llm and the string output passer and that's it now we have a\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='it now we have a rag chain we can then invoke the chain to get an answer to the question what can'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='question what can we do with llama 3 and this answer will then be based on the context which is'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='context which is given by the documents fetched by the retriever moving on to tools so what is a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='tools so what is a tool what is a toolkit and how do we use a tool tools and Lang chain are'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and Lang chain are interfaces that agents chains or llms can use to interact with the world and I'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the world and I think a good way to think about tools is that just like humans need tools to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='need tools to perform tasks llms need tools as well for instance if you want to do some complex'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"to do some complex calculation that you can't do in your head you would need a tool for that a\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='a tool for that a calculator or a computer program and similarly for the llm to be able to perform'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='be able to perform a task it will need access to computational resources or additional data in Lang'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='data in Lang chain you have several built-in tools you have the python Rebel tool you have a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='tool you have a Wikipedia Tool YouTube Tool Saia tool and gradi Tool and so on and more and more'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and more and more tools are constantly being developed Lang chain also has built-in tool kits that'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='tool kits that are essentially collections of tools that are designed to be used to solve a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='be used to solve a specific task and examples of toolkits includes the air toolkit GitHub toolkit G'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='GitHub toolkit G lab toolkit and so on and just like tools the number of tool kits available for'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='kits available for Lang chain is ever increasing so how do we use a tool and there are essentially'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='are essentially two direct ways we can use a tool one is that we can directly run the tool with a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='run the tool with a query and just like loaders this will typically return some data or we can bind'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='data or we can bind the tools to an llm and then we get an llm with access to tools that we can use'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='that we can use in Chains and agents now let me show you how to do that by building on our YouTube'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"on our YouTube example here in the notebook I'm going to pip install YouTube search and then I'm\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"search and then I'm going to import YouTube search tool from Lang chain Community tools and the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"tools and the YouTube search tool allows me to search for YouTube videos so I'll in State the tool\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='in State the tool and then I can run the tool with the query rabid metric and as you can see this'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='as you can see this is going to return a list of videos which are videos from my YouTube channel'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='my YouTube channel now I can take this tool and I can bind it to the llm and then I get an llm with'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='I get an llm with tools and now I can invoke the llm with tools with a query if I do that you can'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='I do that you can see that I get an AI message with some search arguments I can extract tool calls'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='extract tool calls and that will give me a clear overview of the arguments and these are the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and these are the arguments that you need to pass to the YouTube Tool now I can use a Lambda'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='I can use a Lambda function to extract the arguments and then I can compose a chain consisting of'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='chain consisting of my llm with tools the Lambda function and my YouTube Tool and this allows me to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='this allows me to invoke the chain with random queres about YouTube videos so if I ask find some'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='if I ask find some rabid Metric videos on Lang chain you can see that the chain is now returning me'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='is now returning me the list of YouTube videos so this is not much different than just running the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='just running the search but there is now some llm based reasoning behind the crew and this is going'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"and this is going to be more interesting once we start building agents finally let's talk about\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"let's talk about agents I'm going to to explain what an agent is and then we're going to build a\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='going to build a YouTube summary agent what are agents so agents are programs capable of reasoning'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of reasoning and Performing tasks by integrating llms with tools and they do so by leveraging the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"by leveraging the lm's capability to generate Json or other structured output this would be a very\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='would be a very general definition of the concept in Lang chain agents have a very specific meaning'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='specific meaning in Lang chain an agent is a class that uses an llm to choose a sequence of actions'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='sequence of actions to take the main conceptual difference between an agent and a chain is that in'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='a chain is that in a chain the sequence of actions are hardcoded whereas in an agent we let the LM'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='agent we let the LM decide on what actions to take and what tools to use so think of the chain as a'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='of the chain as a fixed sequence of steps whereas the agent might loop back and use the llm to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and use the llm to decide on using a new tool and take a new action and this will go on until the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='go on until the llm decides that the task is completed in theory the agents are more flexible and'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='more flexible and robust than the chains however we also know that LMS can hallucinate so it'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"hallucinate so it depends on the use case now let's build an agent out of our YouTube use case\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"YouTube use case after pip installed Lang chain hop I'm going to use that to extract a pre-made\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"extract a pre-made prompt and I'm going to import hop from Lang chain I'm going to import agent\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='to import agent executor from Lang chain agents and create tool calling agent from Lang chain'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"from Lang chain agents and the pre-made prom I'm going to use is available on the Hub and there you\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"Hub and there you can see the template that's being used and then there's a variable Called Agent\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='Called Agent scratch pad and the agent scratch Pad is an important variable it is where the agent'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='is where the agent will save its intermediary steps but this prompt is ready to use along with our'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='use along with our YouTube Tool and the llm and these are the only three things we need to create'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"we need to create the agent so I'm going to pass the llm the tools the prompt to create tool\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"to create tool calling agent and then I'll set up the agent executor and just like we did with the\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"we did with the chains I can now invoke the agent executor and I'm going to ask the agent to find\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='the agent to find some Lang chain YouTube videos and here we see the list that it extracts with the'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='extracts with the tool and then it ranks the videos and finishes the chain and if we try to click'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='if we try to click on the first video we see that it gets Lang chain expained in 13 minutes by'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='in 13 minutes by rabid metric what a brilliant agent now the cool thing about an agent is that we'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='an agent is that we can just give it access to multiple tools and it will figure out on its own how'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"out on its own how to use the tools I'm now going to create a custom transcribe video tool using\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='video tool using the tool decorator and the YouTube loader we used in section two and this tool'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='two and this tool will take a video URL and return the transcript of the video as a document then'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"as a document then I'm going to create a list of tools with two tools the YouTube search tool which\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='search tool which just used and the transcribe video tool and of course you need to have both'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='need to have both YouTube search and the YouTube transcript API pip installed to do this and now we'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='do this and now we can invoke the agent executor and ask what topics the rabbit metric YouTube'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='metric YouTube channel covers the YouTube agent will now use both tools add its disposal to give me'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='disposal to give me a summary of the topics covered on the channel and here we have the final'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content=\"we have the final output with the summary of the topics covered all right that's it for now if you\"),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='it for now if you want to dive deeper into Lang chain I suggest you go check out some of the more'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='some of the more advanced tutorials on the channel and if you enjoyed this tutorial and want to'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='and want to follow along for more content like this give it a like And subscribe thanks for'),\n",
       " Document(metadata={'source': '8BV9TW490nQ'}, page_content='thanks for watching')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_split = text_splitter.split_documents(docs)\n",
    "docs_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9acb5bee-e42b-4206-94da-45ab537e6cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_password = '2FdJPyocicSwduf9nwVJx4yYX1ndeP12'\n",
    "redis_host = 'redis-19295.c305.ap-south-1-1.ec2.redns.redis-cloud.com'\n",
    "redis_port = '19295'\n",
    "redis_url = 'redis://default:2FdJPyocicSwduf9nwVJx4yYX1ndeP12@redis-19295.c305.ap-south-1-1.ec2.redns.redis-cloud.com:19295'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7a1f04b9-fce9-4e1e-92f7-973b175e7458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cdd9c258-edb4-45d8-bea1-4c63b02d93ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = redis.Redis(\n",
    "    host = redis_host,\n",
    "    port = redis_port,\n",
    "    password = redis_password\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "72b77697-8445-440d-bb0f-b6480213e74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.ping()\n",
    "r.flushdb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3fa39890-e9c2-4094-a582-ba90e7a434d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras==2.13.1Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting tensorflow==2.13.0\n",
      "  Using cached tensorflow-2.13.0-cp311-cp311-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting tensorflow-intel==2.13.0 (from tensorflow==2.13.0)\n",
      "  Using cached tensorflow_intel-2.13.0-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow==2.13.0) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow==2.13.0) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow==2.13.0) (24.3.25)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-intel==2.13.0->tensorflow==2.13.0)\n",
      "  Using cached gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow==2.13.0) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow==2.13.0) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow==2.13.0) (18.1.1)\n",
      "Collecting numpy<=1.24.3,>=1.22 (from tensorflow-intel==2.13.0->tensorflow==2.13.0)\n",
      "  Using cached numpy-1.24.3-cp311-cp311-win_amd64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow==2.13.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow==2.13.0) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow==2.13.0) (3.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow==2.13.0) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow==2.13.0) (1.15.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow==2.13.0) (2.4.0)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-intel==2.13.0->tensorflow==2.13.0)\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow==2.13.0) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow==2.13.0) (1.65.5)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow-intel==2.13.0->tensorflow==2.13.0)\n",
      "  Using cached tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow==2.13.0) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow==2.13.0) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow==2.13.0) (0.41.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow==2.13.0) (2.37.0)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow==2.13.0)\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow==2.13.0) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow==2.13.0) (2.32.3)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow==2.13.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow==2.13.0) (1.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow==2.13.0) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow==2.13.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow==2.13.0) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow==2.13.0) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow==2.13.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow==2.13.0) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow==2.13.0) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow==2.13.0) (2020.6.20)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow==2.13.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\naveena\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow==2.13.0) (3.2.2)\n",
      "Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "Using cached tensorflow-2.13.0-cp311-cp311-win_amd64.whl (1.9 kB)\n",
      "Using cached tensorflow_intel-2.13.0-cp311-cp311-win_amd64.whl (276.6 MB)\n",
      "Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Using cached numpy-1.24.3-cp311-cp311-win_amd64.whl (14.8 MB)\n",
      "Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Installing collected packages: typing-extensions, numpy, keras, gast, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.12.2\n",
      "    Uninstalling typing_extensions-4.12.2:\n",
      "      Successfully uninstalled typing_extensions-4.12.2\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 3.5.0\n",
      "    Uninstalling keras-3.5.0:\n",
      "      Successfully uninstalled keras-3.5.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.6.0\n",
      "    Uninstalling gast-0.6.0:\n",
      "      Successfully uninstalled gast-0.6.0\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 1.2.1\n",
      "    Uninstalling google-auth-oauthlib-1.2.1:\n",
      "      Successfully uninstalled google-auth-oauthlib-1.2.1\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.18.0\n",
      "    Uninstalling tensorboard-2.18.0:\n",
      "      Successfully uninstalled tensorboard-2.18.0\n",
      "  Attempting uninstall: tensorflow-intel\n",
      "    Found existing installation: tensorflow_intel 2.18.0\n",
      "    Uninstalling tensorflow_intel-2.18.0:\n",
      "      Successfully uninstalled tensorflow_intel-2.18.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.18.0\n",
      "    Uninstalling tensorflow-2.18.0:\n",
      "      Successfully uninstalled tensorflow-2.18.0\n",
      "Successfully installed gast-0.4.0 google-auth-oauthlib-1.0.0 keras-2.13.1 numpy-1.24.3 tensorboard-2.13.0 tensorflow-2.13.0 tensorflow-intel-2.13.0 typing-extensions-4.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.3.18 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 1.24.3 which is incompatible.\n",
      "langchain-community 0.3.17 requires numpy<2,>=1.26.4; python_version < \"3.12\", but you have numpy 1.24.3 which is incompatible.\n",
      "langchain-core 0.3.34 requires typing-extensions>=4.7, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "pydantic 2.10.6 requires typing-extensions>=4.12.2, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "pydantic-core 2.27.2 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "pyfume 0.3.4 requires numpy==1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
      "sqlalchemy 2.0.38 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.13.0 which is incompatible.\n",
      "torch 2.4.0 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install keras==2.13.1 tensorflow==2.13.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2649f586-6dbd-41fb-8e23-7a8ad898b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_community.vectorstores.redis import Redis\n",
    "#from langchain.embeddings import HuggingFaceEmbeddings\n",
    "#embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "381281a5-fc0b-4596-83b3-a284e52ec5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.12.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade --quiet sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8712895-085b-455b-a1aa-81f3f8e06625",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d88fc19-28bb-4d2f-b5d7-d3a1b2e2f43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rds = Redis.from_documents(\n",
    "    docs_split,\n",
    "    embeddings,\n",
    "    redis_url = redis_url,\n",
    "    index_name = 'youtube'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d1d698e6-9fb0-4914-88ef-af7e99d63bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "774f5dff-4dea-4ab6-9f40-6bb5860b9fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7da420e6-376e-4a19-9c06-0501e0124694",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install -U langchain langchain-huggingface\n",
    "retriever = rds.as_retriever(search_type = 'similarity', search_kwargs = {'k': 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ea72aa4c-fd52-4d6c-b2b9-8aa5cf505634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': 'doc:youtube:3d9eddce6cf54720acacc1534aef436e', 'source': '8BV9TW490nQ'}, page_content='a quiry and here we get all the chunks of data that are related in some way to data analysis and'),\n",
       " Document(metadata={'id': 'doc:youtube:acabce6c1c544da0b7c968b0eb6c0ad8', 'source': '8BV9TW490nQ'}, page_content='needed to answer the question from your own data or your own documents and to do that a vector'),\n",
       " Document(metadata={'id': 'doc:youtube:eebb2a8514aa4a478e9429cfe90644db', 'source': '8BV9TW490nQ'}, page_content='input some raw data and we get some nice clean data out in a table format and the data cleaning and'),\n",
       " Document(metadata={'id': 'doc:youtube:c7b19a25d11d42ab8f29e49fabd96a60', 'source': '8BV9TW490nQ'}, page_content=\"one is indexing the external data that's what we did in the last section and in the second step we\"),\n",
       " Document(metadata={'id': 'doc:youtube:39ad431e1b8b4362a264daa07d832f5f', 'source': '8BV9TW490nQ'}, page_content=\"optimization operations management and so on all right so let's zoom out a bit and have a look at\"),\n",
       " Document(metadata={'id': 'doc:youtube:becbfae65b8947bbab442fd8eae05e0a', 'source': '8BV9TW490nQ'}, page_content=\"metrics and to do that you need control over the retrieval process which you don't have if you\"),\n",
       " Document(metadata={'id': 'doc:youtube:32965d62b981405d8c3ba9a92771dd16', 'source': '8BV9TW490nQ'}, page_content='saw how the first step works we take some documents we chunk them up we turn them into numeric'),\n",
       " Document(metadata={'id': 'doc:youtube:5d6ca7b0fd49410c845721814ead206d', 'source': '8BV9TW490nQ'}, page_content='with external data and a rag application is built in two steps the first one is indexing the'),\n",
       " Document(metadata={'id': 'doc:youtube:27623000ef1044be81ed2796ada9f884', 'source': '8BV9TW490nQ'}, page_content='of the batch we are retrieving so the density of the relevant search results and in order to'),\n",
       " Document(metadata={'id': 'doc:youtube:b0f2f59ed33144ea99b91f39f8ecad6f', 'source': '8BV9TW490nQ'}, page_content='of some document it could be a recommendation insights from data a piece of executable code or some')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke('data analysis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dcc7b5-69f6-4f64-b8bc-7562c237d717",
   "metadata": {},
   "source": [
    "## Rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "02ec81a6-436f-43ca-bb85-fbc2c4fa8578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "template = \"\"\" Answer the question based only on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "chain = ({\n",
    "    \"context\" : (lambda x: x['question']) | retriever,\n",
    "    \"question\" : (lambda x: x['question'])}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1461d8ec-23d6-4d1f-9080-08e2ec206895",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chain.invoke({\"question\":\"what is langchain\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0e105d88-eb24-4325-8d80-ce5ae6d514f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Based on the provided context, Langchain appears to be a framework or platform for building multi-agent systems. It provides a set of runnable objects and interfaces that agents, chains, or LLMs can use to interact with the world. The core of Langchain is built around Lang graph, which is a graph-based representation of knowledge and beliefs in a multi-agent system.\\n\\nLangchain allows for the creation of chains fast, another feature being its ability to use document loaders, which are interfaces that agents, chains, or LLMs can use to interact with the world. Additionally, Langchain provides tools for building the foundation of a multi-agent system, such as creating runnable objects and understanding the Lang chain expression language in more detail.\\n\\nOverall, Langchain seems to be a powerful framework for building and interacting with multi-agent systems, allowing for efficient creation and manipulation of chains, as well as providing a foundation for understanding the core concepts of Lang chain and its runnable objects.'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35104f05-7aee-4a49-87f2-0e86556656d2",
   "metadata": {},
   "source": [
    "## Chain with Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c468b730-2a95-45d1-9f18-ccb2acd42c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import YouTubeSearchTool\n",
    "\n",
    "youtube_tool = YouTubeSearchTool()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "32a14bb4-c033-45aa-a530-6f30d1c394da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --quiet youtube_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e4dcdcde-79ec-4af7-a167-1fdd13d89675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['https://www.youtube.com/watch?v=8BV9TW490nQ&pp=ygUNUmFiYml0bWV0cmljcw%3D%3D', 'https://www.youtube.com/watch?v=aywZrzNaKjs&pp=ygUNUmFiYml0bWV0cmljcw%3D%3D']\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "youtube_tool.run(\"Rabbitmetrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "51ef927e-93a0-4622-9770-26087abc4d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = llm2.bind_tools([youtube_tool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "64f22f3c-ca5a-4443-b7ff-8ef2fd20efd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = llm_with_tools.invoke(\"Rabbitmetrics YT videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "18b2018a-9bf9-4e0c-8a34-407ca0d8e2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm2 = ChatOllama(\n",
    "    model = 'mistral:latest',\n",
    "    base_url = 'http://localhost:11434',\n",
    "    temperature = 0.3,\n",
    "    format=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bb613ae8-f91b-453f-96a8-a592cdf7eaa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'mistral:latest', 'created_at': '2025-02-23T14:18:23.9916036Z', 'done': True, 'done_reason': 'stop', 'total_duration': 10232550200, 'load_duration': 6694484400, 'prompt_eval_count': 118, 'prompt_eval_duration': 1035000000, 'eval_count': 25, 'eval_duration': 2495000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-8e9917e0-9baa-436a-9d69-312b1eabd2d0-0', tool_calls=[{'name': 'youtube_search', 'args': {'query': 'Rabbitmetrics,10'}, 'id': '39ea6ddc-0658-4a80-8374-12b8c358fc63', 'type': 'tool_call'}], usage_metadata={'input_tokens': 118, 'output_tokens': 25, 'total_tokens': 143})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = llm_with_tools | (lambda x: x.tools_calls[0]['args']['__arg1'] | youtube_tool) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e3b5a23f-fcf5-48bc-a7ae-89d08dd08ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, the answer to the question \"Find some Rabbit metrics videos on Langchain\" is:\\n\\n* Document(metadata={\\'id\\': \\'doc:youtube:131ac41ba9b145ed80d3b45482023460\\', \\'source\\': \\'8BV9TW490nQ\\'}, page_content=\\'do this and now we can invoke the agent executor and ask what topics the rabbit metric YouTube\\')\\n* Document(metadata={\\'id\\': \\'doc:youtube:a520919293d54b3c955ad08d06bca41c\\', \\'source\\': \\'8BV9TW490nQ\\'}, page_content=\\'if I ask find some rabid Metric videos on Lang chain you can see that the chain is now returning me\\')\\n* Document(metadata={\\'id\\': \\'doc:youtube:5068beb0d46e4efc8be1935996aac4dc\\', \\'source\\': \\'8BV9TW490nQ\\'}, page_content=\"surf for separate videos if you\\'re new to Lang chain this tutorial will serve as a quick start and\")\\n* Document(metadata={\\'id\\': \\'doc:youtube:e14d190594c345aeb74ea2f8d26c2728\\', \\'source\\': \\'8BV9TW490nQ\\'}, page_content=\\'the raw transcript and here we have a nice summary of my YouTube video so Lang chain also has a\\')\\n* Document(metadata={\\'id\\': \\'doc:youtube:e910e7ca1f6840d195bf46b6bd4c068b\\', \\'source\\': \\'8BV9TW490nQ\\'}, page_content=\\'an entire video covering that so just head over to the YouTube channel it should be easy to find so\\')\\n* Document(metadata={\\'id\\': \\'doc:youtube:8eca44ca663f491689500a215c2b0b0d\\', \\'source\\': \\'8BV9TW490nQ\\'}, page_content=\\'can also test the chains deploy them monitor the chains and evaluate the output and this can be\\')\\n* Document(metadata={\\'id\\': \\'doc:youtube:b3f558dc9f574addab250937585a70d6\\', \\'source\\': \\'8BV9TW490nQ\\'}, page_content=\\'that can be used in Chains or agents and if you go back and watch some of my earlier videos there\\')\\n* Document(metadata={\\'id\\': \\'doc:youtube:27a00d116301456ca3d2bf61b116dc83\\', \\'source\\': \\'8BV9TW490nQ\\'}, page_content=\\'metric YouTube channel covers the YouTube agent will now use both tools add its disposal to give me\\')\\n* Document(metadata={\\'id\\': \\'doc:youtube:f359f9e8cd794b079879aee59888c09b\\', \\'source\\': \\'8BV9TW490nQ\\'}, page_content=\\'Lang chain but I could also have done this manually using R pi and again I have an entire video\\')\\n* Document(metadata={\\'id\\': \\'doc:youtube:cabc4fe4c6c146cd9c5ad4214f239694\\', \\'source\\': \\'8BV9TW490nQ\\'}, page_content=\"name let\\'s call it YouTube and under the hood Lang chain is using the hogging face embeddings to\")\\n\\nTherefore, the answer to the question \"Find some Rabbit metrics videos on Langchain\" is:\\n\\n* Document(metadata={\\'id\\': \\'doc:youtube:131ac41ba9b145ed80d3b45482023460\\', \\'source\\': \\'8BV9TW490nQ\\'})\\n* Document(metadata={\\'id\\': \\'doc:youtube:a520919293d54b3c955ad08d06bca41c\\', \\'source\\': \\'8BV9TW490nQ\\'})\\n* Document(metadata={\\'id\\': \\'doc:youtube:5068beb0d46e4efc8be1935996aac4dc\\', \\'source\\': \\'8BV9TW490nQ\\'})\\n* Document(metadata={\\'id\\': \\'doc:youtube:e14d190594c345aeb74ea2f8d26c2728\\', \\'source\\': \\'8BV9TW490nQ\\'})\\n* Document(metadata={\\'id\\': \\'doc:youtube:e910e7ca1f6840d195bf46b6bd4c068b\\', \\'source\\': \\'8BV9TW490nQ\\'})\\n* Document(metadata={\\'id\\': \\'doc:youtube:8eca44ca663f4b079879aee59888c09b\\', \\'source\\': \\'8BV9TW490nQ\\'})\\n* Document(metadata={\\'id\\': \\'doc:youtube:27a00d116301456ca3d2bf61b116dc83\\', \\'source\\': \\'8BV9TW490nQ\\'})\\n* Document(metadata={\\'id\\': \\'doc:youtube:f359f9e8cd794b079879aee59888c09b\\', \\'source\\': \\'8BV9TW490nQ\\'})\\n* Document(metadata={\\'id\\': \\'doc:youtube:cabc4fe4c6c146cd9c5ad4214f239694\\', \\'source\\': \\'8BV9TW490nQ\\'})'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"Find some Rabbit metrics videos on langchain\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53084816-b7d2-4faa-8ae4-a6dd5ebdd971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95a05f35-6adc-42c4-8261-d9583c6e62cc",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "455b9a25-3cfb-4c35-98e6-1ddb10339c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, create_tool_calling_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c3837580-ab55-4860-ae15-2c3b20953a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Naveena\\anaconda3\\Lib\\site-packages\\langsmith\\client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull('hwchase17/openai-tools-agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5bd5fa9d-922b-4666-9f1e-0a81f336e481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "da970445-91d6-44de-a00f-dab46b07310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [youtube_tool]\n",
    "agent = create_tool_calling_agent(llm2,tools,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "87c33df0-c207-437c-b252-3ab441fa0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor(agent = agent, tools = tools, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e49b0737-fdad-4798-92dc-f3a6c9ab69f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `youtube_search` with `{'query': 'langchain,10'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m['https://www.youtube.com/watch?v=RoR4XJw8wIc&pp=ygUJbGFuZ2NoYWlu', 'https://www.youtube.com/watch?v=1bUy-1hGZpI&pp=ygUJbGFuZ2NoYWlu', 'https://www.youtube.com/watch?v=nAmC7SoVLd8&pp=ygUJbGFuZ2NoYWlu', 'https://www.youtube.com/watch?v=aywZrzNaKjs&pp=ygUJbGFuZ2NoYWlu', 'https://www.youtube.com/watch?v=swCPic00c30&pp=ygUJbGFuZ2NoYWlu', 'https://www.youtube.com/watch?v=yF9kGESAi3M&pp=ygUJbGFuZ2NoYWlu', 'https://www.youtube.com/watch?v=cQUUkZnyoD0&pp=ygUJbGFuZ2NoYWlu', 'https://www.youtube.com/watch?v=lG7Uxts9SXs&pp=ygUJbGFuZ2NoYWlu', 'https://www.youtube.com/watch?v=sVcwVQRHIc8&pp=ygUJbGFuZ2NoYWlu', 'https://www.youtube.com/watch?v=6puZrTlLHf4&pp=ygUJbGFuZ2NoYWlu']\u001b[0m\u001b[32;1m\u001b[1;3m1. [LangChain: Introduction to the Langchain Platform](https://www.youtube.com/watch?v=RoR4XJw8wIc)\n",
      "\n",
      "2. [LangChain: Building a Language Model with TensorFlow and PyTorch](https://www.youtube.com/watch?v=1bUy-1hGZpI)\n",
      "\n",
      "3. [LangChain: Creating a Custom Language Model from Scratch](https://www.youtube.com/watch?v=nAmC7SoVLd8)\n",
      "\n",
      "4. [LangChain: Fine-Tuning Pretrained Models for Specific Tasks](https://www.youtube.com/watch?v=aywZrzNaKjs)\n",
      "\n",
      "5. [LangChain: Generating Text with a Language Model](https://www.youtube.com/watch?v=swCPic00c30)\n",
      "\n",
      "6. [LangChain: Evaluating the Performance of a Language Model](https://www.youtube.com/watch?v=yF9kGESAi3M)\n",
      "\n",
      "7. [LangChain: Deploying a Language Model to Production](https://www.youtube.com/watch?v=cQUUkZnyoD0)\n",
      "\n",
      "8. [LangChain: Building a Question Answering System with a Language Model](https://www.youtube.com/watch?v=lG7Uxts9SXs)\n",
      "\n",
      "9. [LangChain: Using Langchain for Text Summarization](https://www.youtube.com/watch?v=sVcwVQRHIc8)\n",
      "\n",
      "10. [LangChain: Advanced Topics in Language Modeling](https://www.youtube.com/watch?v=6puZrTlLHf4)\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Find some langchain YT videos',\n",
       " 'output': '1. [LangChain: Introduction to the Langchain Platform](https://www.youtube.com/watch?v=RoR4XJw8wIc)\\n\\n2. [LangChain: Building a Language Model with TensorFlow and PyTorch](https://www.youtube.com/watch?v=1bUy-1hGZpI)\\n\\n3. [LangChain: Creating a Custom Language Model from Scratch](https://www.youtube.com/watch?v=nAmC7SoVLd8)\\n\\n4. [LangChain: Fine-Tuning Pretrained Models for Specific Tasks](https://www.youtube.com/watch?v=aywZrzNaKjs)\\n\\n5. [LangChain: Generating Text with a Language Model](https://www.youtube.com/watch?v=swCPic00c30)\\n\\n6. [LangChain: Evaluating the Performance of a Language Model](https://www.youtube.com/watch?v=yF9kGESAi3M)\\n\\n7. [LangChain: Deploying a Language Model to Production](https://www.youtube.com/watch?v=cQUUkZnyoD0)\\n\\n8. [LangChain: Building a Question Answering System with a Language Model](https://www.youtube.com/watch?v=lG7Uxts9SXs)\\n\\n9. [LangChain: Using Langchain for Text Summarization](https://www.youtube.com/watch?v=sVcwVQRHIc8)\\n\\n10. [LangChain: Advanced Topics in Language Modeling](https://www.youtube.com/watch?v=6puZrTlLHf4)'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"Find some langchain YT videos\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d732cae-9b7a-4639-807f-792f9b77efe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
